<div align="center">

# ğŸš€ MMS Point Cloud Classification with Deep Learning

### Automatic semantic segmentation of Mobile Mapping System data using hierarchical neural networks

[![Status](https://img.shields.io/badge/Status-Completed-success?style=for-the-badge)]()
[![Python](https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-orange?style=for-the-badge&logo=pytorch)]()
[![CUDA](https://img.shields.io/badge/CUDA-12.1-green?style=for-the-badge&logo=nvidia)]()
[![License](https://img.shields.io/badge/License-Educational-purple?style=for-the-badge)]()

---

### ğŸ¯ **Final Results: 94.78% Accuracy**

| Metric | Value | Status |
|:------:|:-----:|:------:|
| **Overall Accuracy** | **94.78%** | âœ… Exceeded target (88-90%) |
| **Mean IoU** | **87.51%** | âœ… Excellent |
| **Kappa Coefficient** | **0.9187** | âœ… Outstanding |
| **Test Points** | **219,168** | âœ… Robust evaluation |

</div>

---

## ğŸ“‹ Table of Contents

- [ğŸŒŸ Highlights](#-highlights)
- [ğŸ¥ Demo & Visualizations](#-demo--visualizations)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ“Š Dataset](#-dataset)
- [ğŸ—ï¸ Model Architectures](#ï¸-model-architectures)
- [ğŸ”¬ Evaluation Metrics](#-evaluation-metrics)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ’» Installation](#-installation)
- [ğŸš€ Usage](#-usage)
- [ğŸ“ Technical Details](#-technical-details)
- [ğŸ† Achievements](#-achievements)
- [ğŸ› ï¸ Troubleshooting](#ï¸-troubleshooting)
- [ğŸ“š References](#-references)
- [ğŸ“… Project Timeline](#-project-timeline)

---

## ğŸŒŸ Highlights

<div align="center">

```
ğŸ¯ EXCEEDED TARGET: 94.78% accuracy vs 88-90% goal (+4.78% to +6.78%)
ğŸ“ˆ MAJOR IMPROVEMENT: +8.77% accuracy over baseline (SimplePointNet)
ğŸš€ GPU OPTIMIZED: Training time reduced from 24+ hours (CPU) to 4 hours (GPU)
ğŸ¨ MULTI-SCALE: Hierarchical PointNet++ captures fine details and global context
ğŸ“¦ 1.46M POINTS: Comprehensive dataset across 5 semantic categories
```

</div>

### What This Project Does

Automatically classifies **1.46 million 3D points** from Mobile Mapping System (MMS) street scans into 5 semantic categories:

| Category | Description | Examples | IoU Performance |
|:--------:|-------------|----------|:---------------:|
| ğŸ›£ï¸ **Road** | Road surfaces, ground, bridge decks | Asphalt, concrete, pavement | **91.45%** |
| â„ï¸ **Snow** | Snow coverage on any surface | Fresh snow, ice, slush | **91.87%** |
| ğŸš— **Vehicle** | Cars, trucks, and other vehicles | Sedans, SUVs, trucks | **79.15%** |
| ğŸŒ³ **Vegetation** | Low, medium, and high vegetation | Trees, bushes, grass | **85.30%** |
| ğŸ¢ **Others** | Buildings, unclassified objects | Walls, signs, poles | **89.75%** |

### Real-World Applications

- ğŸš™ **Autonomous Vehicles**: Scene understanding for self-driving cars
- â„ï¸ **Winter Maintenance**: Automatic snow detection on roads
- ğŸ™ï¸ **Urban Planning**: Infrastructure mapping and asset management
- ğŸŒ¿ **Environmental Monitoring**: Vegetation tracking and analysis

---

## ğŸ¥ Demo & Visualizations

### Model Comparison: SimplePointNet vs PointNet++

<div align="center">

**PointNet++ achieves +8.77% accuracy improvement!**

</div>

**Overall Metrics:**
```
                    SimplePointNet    PointNet++     Improvement
Overall Accuracy        86.01%         94.78%         +8.77%
Mean IoU               75.79%         87.51%        +11.72%
Kappa Coefficient       0.7742         0.9187        +0.1445
```

**Biggest Improvements:**
- ğŸŒ³ **Vegetation**: +24.04% IoU (61.26% â†’ 85.30%)
- â„ï¸ **Snow**: +20.37% IoU (71.50% â†’ 91.87%)
- ğŸ›£ï¸ **Road**: +18.76% IoU (72.69% â†’ 91.45%)

### Confusion Matrix (PointNet++)

See `results/pointnet2_confusion_matrix_normalized.png` for detailed visualization.

**Key Findings:**
- âœ… **Snow**: 95.53% recall (excellent detection)
- âœ… **Vegetation**: 97.52% recall (very few false negatives)
- âœ… **Road**: 99.49% precision (very few false positives)
- âš ï¸ **Vehicle**: 79.15% IoU (challenging due to class imbalance - only 2.2% of data)

### Training Progress

**PointNet++ Training (30 epochs on RTX 4050):**
```
Epoch  1/30: Train Loss=1.234, Val IoU=60.2%  â¬›â¬›â¬›â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
Epoch  5/30: Train Loss=0.456, Val IoU=75.8%  â¬›â¬›â¬›â¬›â¬›â¬›â¬œâ¬œâ¬œâ¬œ
Epoch 10/30: Train Loss=0.278, Val IoU=85.3%  â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬œâ¬œ
Epoch 20/30: Train Loss=0.152, Val IoU=93.1%  â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬œ
Epoch 28/30: Train Loss=0.089, Val IoU=94.05% â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬› â† BEST!
Epoch 30/30: Train Loss=0.075, Val IoU=93.9%  â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬œ

Final Test Accuracy: 94.78%
```

---

## âš¡ Quick Start

### ğŸ¯ Inference on Your Data (5 minutes)

```python
import torch
import numpy as np
from models.pointnet2 import PointNet2

# 1. Load trained model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = PointNet2(num_classes=5, num_features=7).to(device)
checkpoint = torch.load('checkpoints/pointnet2_best_model.pth', map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# 2. Prepare your point cloud (N points Ã— 7 features: X,Y,Z,R,G,B,Intensity)
# xyz: (N, 3) - coordinates
# features: (N, 7) - [X, Y, Z, R, G, B, Intensity]

# 3. Normalize XYZ
xyz_norm = (xyz - xyz.mean(axis=0)) / (xyz.std() + 1e-8)
features[:, :3] = xyz_norm

# 4. Run inference
coords_tensor = torch.from_numpy(xyz_norm).float().unsqueeze(0).to(device)
features_tensor = torch.from_numpy(features).float().unsqueeze(0).to(device)

with torch.no_grad():
    logits = model(coords_tensor, features_tensor)
    predictions = torch.argmax(logits, dim=2).cpu().numpy().flatten()

# 5. Class mapping
classes = {0: "Road", 1: "Snow", 2: "Vehicle", 3: "Vegetation", 4: "Others"}
```

### ğŸ‹ï¸ Training from Scratch (4 hours on GPU)

```bash
# 1. Install dependencies
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install numpy matplotlib seaborn scikit-learn laspy tqdm

# 2. Prepare data from LAS files
python prepare_training_data.py

# 3. Train PointNet++ model
python train_pointnet2.py

# 4. Evaluate on test set
python evaluate_pointnet2.py
```

**Expected Output:**
```
Training PointNet++ on RTX 4050...
Epoch 28/30: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Train Loss: 0.089, Val IoU: 94.05%
âœ“ New best model saved! (Epoch 28, Val IoU: 94.05%)

Test Results:
  Overall Accuracy: 94.78%
  Mean IoU: 87.51%
  Kappa: 0.9187

Per-Class IoU:
  Road:       91.45%
  Snow:       91.87%
  Vehicle:    79.15%
  Vegetation: 85.30%
  Others:     89.75%
```

---

## ğŸ“Š Dataset

### Overview

| Metric | Value |
|--------|-------|
| **Total Points** | 1,461,189 |
| **Features per Point** | 7 (XYZ + RGB + Intensity) |
| **Classes** | 5 semantic categories |
| **Source Format** | LAS (CloudCompare labeled) |
| **Data Split** | 70% train / 15% val / 15% test |

### Class Distribution

```
Snow (47.1%)         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Others (35.1%)       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Vegetation (10.6%)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Road (5.0%)          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Vehicle (2.2%)       â–ˆâ–ˆ
```

**Class Balance Considerations:**
- âš ï¸ **Vehicle** is heavily imbalanced (2.2% of data)
  - **Impact**: Lower IoU (79.15%) compared to other classes
  - **Future improvement**: Class weighting, focal loss, or oversampling
- âœ… Other classes have sufficient representation

### Features

Each point has **7 features**:

1. **X, Y, Z** - 3D coordinates (normalized during preprocessing)
2. **R, G, B** - Color values (scaled to 0-1 range)
3. **Intensity** - LiDAR intensity (scaled to 0-1 range)

### Data Preprocessing

```python
# Normalization pipeline:
1. Center XYZ coordinates (subtract mean)
2. Scale XYZ by standard deviation
3. Scale RGB and Intensity to [0, 1]
4. Random rotation (Z-axis) for training augmentation
5. Random scaling (0.95-1.05) for training augmentation
```

---

## ğŸ—ï¸ Model Architectures

### âœ… PointNet++ (Recommended)

<div align="center">

**ğŸ† Best Model: 94.78% Accuracy**

</div>

**Architecture Overview:**
```
Input: [Batch, 2048 points, 7 features]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENCODER (Hierarchical Downsampling) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SA1: 2048 â†’ 1024 pts, r=0.1m, 64-D  â”‚
â”‚ SA2: 1024 â†’ 256 pts,  r=0.2m, 128-D â”‚
â”‚ SA3: 256 â†’ 64 pts,    r=0.4m, 256-D â”‚
â”‚ SA4: 64 â†’ 16 pts,     r=0.8m, 512-D â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECODER (Feature Propagation)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FP4: 16 â†’ 64 pts,   256-D + skip    â”‚
â”‚ FP3: 64 â†’ 256 pts,  256-D + skip    â”‚
â”‚ FP2: 256 â†’ 1024 pts, 128-D + skip   â”‚
â”‚ FP1: 1024 â†’ 2048 pts, 128-D + skip  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output: [Batch, 2048 points, 5 classes]
```

**Key Features:**
- âœ… **Multi-scale learning**: Captures both fine details (0.1m) and global context (0.8m)
- âœ… **Skip connections**: Preserves spatial details during upsampling
- âœ… **Hierarchical**: 4 encoder levels + 4 decoder levels
- âœ… **Parameters**: 968,069 (moderate size)

**Performance:**
- Overall Accuracy: **94.78%**
- Mean IoU: **87.51%**
- Training Time: **~4 hours** (RTX 4050 GPU)
- Best for: **Highest accuracy**, critical applications

**When to Use:**
- âœ… You need the best accuracy (94.78%)
- âœ… Vegetation and Snow classification are important
- âœ… You have GPU resources (4 hours training)
- âœ… Production deployment with quality requirements

---

### âœ… SimplePointNet (Baseline)

<div align="center">

**ğŸ“Š Baseline: 86.01% Accuracy**

</div>

**Architecture Overview:**
```
Input: [Batch, 2048 points, 7 features]
    â†“
Shared MLP: [64, 128, 1024]
    â†“
Global Max Pooling â†’ [Batch, 1024]
    â†“
Expand to all points â†’ [Batch, 2048, 1024]
    â†“
Concatenate with features â†’ [Batch, 2048, 1031]
    â†“
Shared MLP: [512, 256, 128]
    â†“
Output: [Batch, 2048, 5 classes]
```

**Key Features:**
- âœ… **Single-scale**: Global features only
- âœ… **Fast training**: 2.5 hours on GPU
- âœ… **Lightweight**: 192,517 parameters
- âœ… **Good baseline**: 86% accuracy

**Performance:**
- Overall Accuracy: **86.01%**
- Mean IoU: **75.79%**
- Training Time: **~2.5 hours** (RTX 4050 GPU)
- Best for: **Quick experiments**, resource constraints

**When to Use:**
- âœ… Fast prototyping
- âœ… Limited GPU memory (<4GB)
- âœ… Can accept 86% accuracy
- âœ… Baseline for comparison

---

### âš ï¸ RandLA-Net (Future Work)

**Status:** Implementation incomplete (torch.gather dimension mismatch)

**Potential Benefits:**
- Designed for large-scale outdoor scenes (millions of points)
- Memory-efficient random sampling
- State-of-the-art on SemanticKITTI, Semantic3D benchmarks

**Future Work:**
- Debug dimension mismatch
- Complete implementation
- Compare with PointNet++

---

## ğŸ”¬ Evaluation Metrics

### Overall Performance

<div align="center">

| Model | Accuracy | Mean IoU | Kappa | F1-Score | Parameters | Training Time |
|:-----:|:--------:|:--------:|:-----:|:--------:|:----------:|:-------------:|
| **SimplePointNet** | 86.01% | 75.79% | 0.7742 | 85.99% | 192K | 2.5h |
| **PointNet++** | **94.78%** | **87.51%** | **0.9187** | **94.79%** | 968K | 4h |
| **Improvement** | **+8.77%** | **+11.72%** | **+0.1445** | **+8.80%** | +776K | +1.5h |

</div>

### Per-Class Performance (PointNet++)

| Class | IoU | Precision | Recall | F1-Score | Test Points | Notes |
|:-----:|:---:|:---------:|:------:|:--------:|:-----------:|:------|
| ğŸ›£ï¸ **Road** | 91.45% | 99.49% | 91.89% | 95.54% | 11,029 | Excellent precision |
| â„ï¸ **Snow** | 91.87% | 96.00% | 95.53% | 95.77% | 103,140 | Largest class, best overall |
| ğŸš— **Vehicle** | 79.15% | 97.74% | 80.62% | 88.36% | 4,836 | Class imbalance challenge |
| ğŸŒ³ **Vegetation** | 85.30% | 87.19% | 97.52% | 92.07% | 23,233 | High recall |
| ğŸ¢ **Others** | 89.75% | 94.94% | 94.26% | 94.60% | 76,951 | Balanced performance |

**Key Insights:**
- âœ… **Snow and Road**: Excellent performance (>91% IoU)
- âœ… **High Precision**: Vehicle (97.74%), Road (99.49%) - few false positives
- âœ… **High Recall**: Vegetation (97.52%) - few false negatives
- âš ï¸ **Vehicle Challenge**: Lowest IoU (79.15%) due to class imbalance (only 2.2% of data)

### Metric Definitions

**IoU (Intersection over Union):**
```
IoU = True Positives / (True Positives + False Positives + False Negatives)
```
- Standard metric for segmentation tasks
- Range: 0-100% (higher is better)
- Accounts for both precision and recall

**Kappa Coefficient:**
```
Kappa = (Observed Agreement - Expected Agreement) / (1 - Expected Agreement)
```
- Measures agreement beyond chance
- Range: 0-1 (0.9187 = excellent agreement)
- More robust than accuracy for imbalanced data

**Precision vs Recall:**
- **Precision**: Of all predicted positives, how many are correct?
- **Recall**: Of all actual positives, how many did we find?
- **F1-Score**: Harmonic mean of precision and recall

---

## ğŸ“‚ Project Structure

```
LAB PROJECT/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ sample1.las                      # Original CloudCompare-labeled LAS
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train_data.npz                   # 1,022,832 points (70%)
â”‚       â”œâ”€â”€ val_data.npz                     # 219,189 points (15%)
â”‚       â””â”€â”€ test_data.npz                    # 219,168 points (15%)
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ simple_pointnet.py                   # Baseline (86% accuracy)
â”‚   â”œâ”€â”€ pointnet2.py                         # PointNet++ (94.78% accuracy) â­
â”‚   â””â”€â”€ randlanet.py                         # Future work (incomplete)
â”‚
â”œâ”€â”€ ğŸ“ evaluation/
â”‚   â””â”€â”€ metrics.py                           # IoU, Kappa, F1, confusion matrix
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/
â”‚   â”œâ”€â”€ pointnet2_best_model.pth            # Best PointNet++ (epoch 28) â­
â”‚   â”œâ”€â”€ best_model.pth                       # Best SimplePointNet
â”‚   â”œâ”€â”€ pointnet2_training_history.json     # Training logs
â”‚   â””â”€â”€ training_history.json               # Baseline training logs
â”‚
â”œâ”€â”€ ğŸ“ results/
â”‚   â”œâ”€â”€ pointnet2_test_results.json         # Final test metrics â­
â”‚   â”œâ”€â”€ pointnet2_confusion_matrix.png      # Confusion matrix (raw)
â”‚   â”œâ”€â”€ pointnet2_confusion_matrix_normalized.png  # Confusion matrix (%)
â”‚   â”œâ”€â”€ model_comparison.md                 # Detailed comparison report
â”‚   â”œâ”€â”€ overall_metrics_comparison.png      # Bar chart
â”‚   â”œâ”€â”€ per_class_iou_comparison.png        # Per-class IoU
â”‚   â”œâ”€â”€ per_class_f1_comparison.png         # Per-class F1
â”‚   â”œâ”€â”€ precision_recall_scatter.png        # Precision vs Recall
â”‚   â”œâ”€â”€ improvement_heatmap.png             # PointNet++ improvements
â”‚   â””â”€â”€ model_summary_table.png             # Summary table
â”‚
â”œâ”€â”€ ğŸ“„ prepare_training_data.py              # Data preprocessing
â”œâ”€â”€ ğŸ“„ train_pointnet2.py                    # PointNet++ training â­
â”œâ”€â”€ ğŸ“„ train_from_processed.py               # SimplePointNet training
â”œâ”€â”€ ğŸ“„ evaluate_pointnet2.py                 # PointNet++ evaluation â­
â”œâ”€â”€ ğŸ“„ evaluate_model.py                     # SimplePointNet evaluation
â”œâ”€â”€ ğŸ“„ create_comparison_visualizations.py   # Generate comparison charts
â”œâ”€â”€ ğŸ“„ check_training.py                     # Monitor training progress
â”œâ”€â”€ ğŸ“„ class_mapping_config.py               # LAS class mapping
â”‚
â”œâ”€â”€ ğŸ“„ YOUR_COMPLETE_PROJECT_GUIDE.md       # Comprehensive 80+ page guide
â”œâ”€â”€ ğŸ“„ FINAL_PROJECT_SUMMARY.md             # Executive summary
â”œâ”€â”€ ğŸ“„ README.md                             # This file
â””â”€â”€ ğŸ“„ requirements.txt                      # Python dependencies
```

---

## ğŸ’» Installation

### System Requirements

- **OS**: Windows, Linux, or macOS
- **Python**: 3.8 or higher
- **GPU** (highly recommended): NVIDIA RTX 3060 or better
  - **Minimum VRAM**: 6GB (tested on RTX 4050)
  - **CUDA**: 11.8 or 12.1
- **RAM**: 16GB minimum, 32GB recommended
- **Storage**: 5GB for data + models

### Step 1: Clone Repository

```bash
git clone https://github.com/nikengoswami/MMS-Point-Cloud-Classification.git
cd MMS-Point-Cloud-Classification
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Using conda
conda create -n pointcloud python=3.10
conda activate pointcloud

# OR using venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# OR
venv\Scripts\activate  # Windows
```

### Step 3: Install PyTorch with CUDA

**For CUDA 12.1 (recommended):**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**For CUDA 11.8:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**For CPU only (not recommended for training):**
```bash
pip install torch torchvision torchaudio
```

### Step 4: Install Other Dependencies

```bash
pip install numpy matplotlib seaborn scikit-learn laspy tqdm
```

**OR use requirements file:**
```bash
pip install -r requirements.txt
```

### Step 5: Verify GPU Detection

```bash
python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')"
```

**Expected output:**
```
CUDA available: True
GPU: NVIDIA GeForce RTX 4050 Laptop GPU
```

---

## ğŸš€ Usage

### 1ï¸âƒ£ Data Preparation

#### Option A: Use Pre-processed Data (Recommended for Quick Start)

Download pre-processed data from releases:
```bash
# Download train_data.npz, val_data.npz, test_data.npz
# Place in data/processed/
```

#### Option B: Process Your Own LAS Files

**Step 1: Label in CloudCompare**

1. Open your LAS file in CloudCompare
2. Use "Segment" tool to select regions
3. Assign classification codes:
   - `0` or `11`: Road / Ground
   - `1`: Snow
   - `2`: Vehicle
   - `3-5`: Vegetation (Low/Medium/High)
   - `6`: Others / Building
4. Save labeled file to `data/raw/sample1.las`

**Step 2: Run Preprocessing**

```bash
python prepare_training_data.py
```

**Output:**
```
Loading LAS file: data/raw/sample1.las
Total points loaded: 1,461,189
Applying class mapping...
  Road: 72,513 points (5.0%)
  Snow: 687,221 points (47.1%)
  Vehicle: 31,854 points (2.2%)
  Vegetation: 154,680 points (10.6%)
  Others: 514,921 points (35.1%)

Splitting data (70/15/15)...
  Train: 1,022,832 points
  Val: 219,189 points
  Test: 219,168 points

Saving to data/processed/...
âœ“ Complete!
```

---

### 2ï¸âƒ£ Training

#### Train PointNet++ (Recommended)

```bash
python train_pointnet2.py
```

**Training Configuration:**
```python
batch_size = 8              # Adjust based on GPU memory
num_points = 2048           # Points per sample
learning_rate = 0.001       # Adam optimizer
num_epochs = 30             # Total epochs
augmentation = True         # Random rotation + scaling
scheduler = 'plateau'       # ReduceLROnPlateau (patience=5)
```

**Live Training Output:**
```
Epoch 1/30
Training:   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [08:23<00:00]
  Train Loss: 1.234, Train Acc: 45.2%
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [01:42<00:00]
  Val Loss: 1.156, Val IoU: 60.2%

Epoch 10/30
Training:   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [07:58<00:00]
  Train Loss: 0.278, Train Acc: 89.5%
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [01:35<00:00]
  Val Loss: 0.312, Val IoU: 85.3%
âœ“ New best model saved!

Epoch 28/30
Training:   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [07:45<00:00]
  Train Loss: 0.089, Train Acc: 97.2%
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [01:33<00:00]
  Val Loss: 0.198, Val IoU: 94.05%
âœ“ New best model saved! (Best so far)

Training complete! Best Val IoU: 94.05% (Epoch 28)
Model saved to: checkpoints/pointnet2_best_model.pth
```

**Monitor Progress in Real-Time:**
```bash
# In another terminal
python check_training.py

# OR watch logs
tail -f pointnet2_training.log
```

#### Train SimplePointNet (Baseline)

```bash
python train_from_processed.py
```

**Faster training (~2.5 hours) but lower accuracy (86%)**

---

### 3ï¸âƒ£ Evaluation

#### Evaluate PointNet++ on Test Set

```bash
python evaluate_pointnet2.py
```

**Evaluation Output:**
```
Loading test data: data/processed/test_data.npz
Test set: 219,168 points

Loading PointNet++ model...
Loaded checkpoint from epoch 28

Running inference...
Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:15<00:00]

Computing metrics...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           POINTNET++ TEST RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Overall Metrics:
  Accuracy:           94.78%
  Mean IoU:           87.51%
  Kappa Coefficient:  0.9187
  Weighted F1-Score:  0.9479

Per-Class Performance:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Class      â”‚ IoU    â”‚ Precision â”‚ Recall â”‚ F1-Score â”‚ Support â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Road       â”‚ 91.45% â”‚   99.49%  â”‚ 91.89% â”‚  95.54%  â”‚ 11,029  â”‚
â”‚ Snow       â”‚ 91.87% â”‚   96.00%  â”‚ 95.53% â”‚  95.77%  â”‚ 103,140 â”‚
â”‚ Vehicle    â”‚ 79.15% â”‚   97.74%  â”‚ 80.62% â”‚  88.36%  â”‚  4,836  â”‚
â”‚ Vegetation â”‚ 85.30% â”‚   87.19%  â”‚ 97.52% â”‚  92.07%  â”‚ 23,233  â”‚
â”‚ Others     â”‚ 89.75% â”‚   94.94%  â”‚ 94.26% â”‚  94.60%  â”‚ 76,951  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Results saved to: results/pointnet2_test_results.json
Confusion matrix saved to: results/pointnet2_confusion_matrix.png
```

#### Generate Model Comparison Charts

```bash
python create_comparison_visualizations.py
```

**Generates 6 visualizations:**
1. `overall_metrics_comparison.png` - Bar chart (Accuracy, IoU, Kappa)
2. `per_class_iou_comparison.png` - Per-class IoU comparison
3. `per_class_f1_comparison.png` - Per-class F1-score comparison
4. `precision_recall_scatter.png` - Precision vs Recall plot
5. `improvement_heatmap.png` - PointNet++ improvement heatmap
6. `model_summary_table.png` - Summary table image

---

### 4ï¸âƒ£ Inference on New Data

**Example: Classify a new point cloud**

```python
import torch
import numpy as np
import laspy
from models.pointnet2 import PointNet2

# 1. Load your LAS file
las = laspy.read('your_data.las')
xyz = np.vstack([las.x, las.y, las.z]).T
rgb = np.vstack([las.red, las.green, las.blue]).T / 65535.0  # Normalize
intensity = las.intensity.reshape(-1, 1) / 255.0

# Combine features
features = np.hstack([xyz, rgb, intensity])  # Shape: (N, 7)

# 2. Load trained model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = PointNet2(num_classes=5, num_features=7).to(device)
checkpoint = torch.load('checkpoints/pointnet2_best_model.pth', map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# 3. Normalize XYZ
xyz_mean = xyz.mean(axis=0)
xyz_std = xyz.std(axis=0)
xyz_norm = (xyz - xyz_mean) / (xyz_std + 1e-8)
features[:, :3] = xyz_norm

# 4. Process in batches (for large point clouds)
batch_size = 8192
predictions = []

for i in range(0, len(xyz), batch_size):
    batch_xyz = xyz_norm[i:i+batch_size]
    batch_features = features[i:i+batch_size]

    # Convert to tensors
    coords = torch.from_numpy(batch_xyz).float().unsqueeze(0).to(device)
    feats = torch.from_numpy(batch_features).float().unsqueeze(0).to(device)

    # Predict
    with torch.no_grad():
        logits = model(coords, feats)
        preds = torch.argmax(logits, dim=2).cpu().numpy().flatten()

    predictions.append(preds)

# 5. Combine predictions
all_predictions = np.concatenate(predictions)

# 6. Map to class names
class_names = {0: "Road", 1: "Snow", 2: "Vehicle", 3: "Vegetation", 4: "Others"}
print(f"Classified {len(all_predictions)} points")
for i in range(5):
    count = (all_predictions == i).sum()
    print(f"  {class_names[i]}: {count} points ({count/len(all_predictions)*100:.1f}%)")
```

---

## ğŸ“ Technical Details

### PointNet++ Architecture Deep Dive

**Set Abstraction (SA) Module:**
```python
def set_abstraction(xyz, features, num_samples, radius, mlp_layers):
    """
    Args:
        xyz: (B, N, 3) - Point coordinates
        features: (B, N, C) - Point features
        num_samples: Number of points to sample (FPS)
        radius: Ball query radius
        mlp_layers: MLP output dimensions

    Returns:
        new_xyz: (B, num_samples, 3) - Sampled coordinates
        new_features: (B, num_samples, C') - Aggregated features
    """
    # 1. Farthest Point Sampling (FPS)
    centroids = farthest_point_sampling(xyz, num_samples)

    # 2. Ball Query (find neighbors within radius)
    neighbors = ball_query(xyz, centroids, radius, max_neighbors=32)

    # 3. PointNet on local neighborhood
    local_features = pointnet_on_groups(neighbors, features, mlp_layers)

    # 4. Max pooling across neighbors
    aggregated = max_pool(local_features, dim=neighbors)

    return centroids, aggregated
```

**Feature Propagation (FP) Module:**
```python
def feature_propagation(xyz1, xyz2, features1, features2, mlp_layers):
    """
    Args:
        xyz1: (B, N1, 3) - Sparse coordinates (from encoder)
        xyz2: (B, N2, 3) - Dense coordinates (target)
        features1: (B, N1, C1) - Sparse features
        features2: (B, N2, C2) - Skip connection features

    Returns:
        interpolated_features: (B, N2, C') - Upsampled features
    """
    # 1. Inverse distance weighted interpolation
    interpolated = interpolate_3nn(xyz1, xyz2, features1)

    # 2. Concatenate with skip connection
    if features2 is not None:
        combined = torch.cat([interpolated, features2], dim=-1)
    else:
        combined = interpolated

    # 3. Refine with MLP
    refined = mlp(combined, mlp_layers)

    return refined
```

### Training Details

**Data Augmentation:**
```python
# Random Z-axis rotation (0-360 degrees)
angle = np.random.uniform(0, 2 * np.pi)
rotation_matrix = np.array([
    [np.cos(angle), -np.sin(angle), 0],
    [np.sin(angle),  np.cos(angle), 0],
    [0,              0,             1]
])
xyz_rotated = xyz @ rotation_matrix

# Random scaling (95%-105%)
scale = np.random.uniform(0.95, 1.05)
xyz_scaled = xyz_rotated * scale
```

**Loss Function:**
```python
# Cross-entropy loss with class weights (optional)
criterion = nn.CrossEntropyLoss()

# Forward pass
logits = model(xyz, features)  # (B, N, C)
logits_flat = logits.reshape(-1, num_classes)  # (B*N, C)
labels_flat = labels.reshape(-1)  # (B*N)

# Compute loss
loss = criterion(logits_flat, labels_flat)
```

**Optimizer & Scheduler:**
```python
# Adam optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Learning rate scheduler (reduce on plateau)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',          # Monitor validation IoU (maximize)
    factor=0.5,          # Reduce LR by 50%
    patience=5,          # Wait 5 epochs before reducing
    verbose=True
)
```

### Class Mapping Configuration

```python
# LAS standard codes â†’ Our 5 target classes
LAS_TO_TARGET = {
    0: 4,   # Never classified â†’ Others
    1: 4,   # Unclassified â†’ Others
    2: 0,   # Ground â†’ Road
    3: 3,   # Low Vegetation â†’ Vegetation
    4: 3,   # Medium Vegetation â†’ Vegetation
    5: 3,   # High Vegetation â†’ Vegetation
    6: 4,   # Building â†’ Others
    7: 4,   # Low Point (noise) â†’ Others
    9: 4,   # Water â†’ Others
    10: 4,  # Rail â†’ Others
    11: 0,  # Road Surface â†’ Road
    17: 0,  # Bridge Deck â†’ Road
}

TARGET_CLASSES = {
    0: "Road",
    1: "Snow",
    2: "Vehicle",
    3: "Vegetation",
    4: "Others"
}
```

---

## ğŸ† Achievements

### Technical Achievements âœ…

<div align="center">

| Achievement | Details | Status |
|:-----------:|---------|:------:|
| **ğŸ¯ Exceeded Target** | 94.78% vs 88-90% goal (+4.78% to +6.78%) | âœ… |
| **ğŸš€ GPU Acceleration** | 6Ã— speedup (24h â†’ 4h) | âœ… |
| **ğŸ”§ Fixed PointNet++** | Resolved dimension mismatch, tensor format issues | âœ… |
| **ğŸ“Š Comprehensive Eval** | 11 visualizations, detailed metrics | âœ… |
| **ğŸ’¾ Data Pipeline** | 1.46M points labeled, processed, split | âœ… |
| **ğŸ“ˆ Major Improvement** | +24% Vegetation IoU, +20% Snow IoU | âœ… |

</div>

### Key Fixes Applied

**1. PointNet++ Dimension Mismatch (Line 232):**
```python
# BEFORE (BROKEN):
self.sa1 = PointNetSetAbstraction(in_channel=num_features, ...)  # 7

# AFTER (FIXED):
self.sa1 = PointNetSetAbstraction(in_channel=num_features + 3, ...)  # 10
# Reason: SA layer concatenates XYZ (3) with features (7) = 10 channels
```

**2. Tensor Format Compatibility:**
```python
# Added permutations between encoder/decoder:
l1_points = l1_points.permute(0, 2, 1)  # (B, C, N) â†’ (B, N, C)
```

**3. GPU Installation:**
```bash
# Installed CUDA-enabled PyTorch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

### Performance Breakdown

**What Went Right:**
- âœ… Snow classification: 91.87% IoU (excellent white/high-intensity detection)
- âœ… Road classification: 91.45% IoU (excellent flat surface detection)
- âœ… Vegetation: +24% IoU improvement over baseline (multi-scale helps organic shapes)
- âœ… Overall robust performance across all classes

**What's Challenging:**
- âš ï¸ Vehicle class: 79.15% IoU
  - **Reason**: Only 2.2% of data (severe class imbalance)
  - **Small objects**: 100-500 points per vehicle vs 10,000+ for other classes
  - **Future fix**: Class weighting, focal loss, or oversampling

**Comparison to Literature:**
- âœ… 94.78% accuracy on custom dataset (excellent)
- âœ… 87.51% mean IoU (competitive with published results)
- âœ… SemanticKITTI benchmark: ~60-75% mIoU (outdoor scenes)
- âœ… Our result (87.51%) exceeds typical outdoor scene segmentation

---

## ğŸ› ï¸ Troubleshooting

### Common Issues & Solutions

#### Issue 1: CUDA Out of Memory

**Error:**
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```

**Solutions:**
```python
# Option 1: Reduce batch size
batch_size = 4  # Instead of 8

# Option 2: Reduce points per sample
num_points = 1024  # Instead of 2048

# Option 3: Clear cache
torch.cuda.empty_cache()

# Option 4: Use gradient accumulation
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

#### Issue 2: CUDA Not Detected

**Error:**
```
GPU: Not detected
```

**Check:**
```bash
# Verify CUDA installation
nvidia-smi

# Check PyTorch CUDA
python -c "import torch; print(torch.version.cuda)"
```

**Solutions:**
```bash
# Reinstall PyTorch with CUDA
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Verify again
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

---

#### Issue 3: Dimension Mismatch

**Error:**
```
RuntimeError: mat1 and mat2 shapes cannot be multiplied
```

**Check:**
```python
# Add debug prints in model forward():
print(f"SA1 input: {xyz.shape}, {features.shape}")
print(f"SA1 output: {l1_xyz.shape}, {l1_points.shape}")
print(f"FP1 input: {xyz.shape}, {features.shape}")
```

**Common cause:** Features not properly concatenated with XYZ

---

#### Issue 4: Slow Training (CPU)

**Issue:** Training taking 24+ hours

**Solution:**
```bash
# Verify GPU is being used
python -c "import torch; print('Device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')"

# If CPU detected:
# 1. Install CUDA PyTorch (see Issue 2)
# 2. Ensure model.to(device) is called
# 3. Ensure data.to(device) is called in training loop
```

---

#### Issue 5: NaN Loss During Training

**Error:**
```
Epoch 5: Loss = NaN
```

**Solutions:**
```python
# Option 1: Reduce learning rate
learning_rate = 0.0001  # Instead of 0.001

# Option 2: Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Option 3: Check data
# Ensure no NaN/Inf in input data
assert not torch.isnan(xyz).any()
assert not torch.isinf(xyz).any()

# Option 4: Add epsilon to normalization
xyz_norm = (xyz - mean) / (std + 1e-8)
```

---

#### Issue 6: Low Accuracy (< 80%)

**Possible causes:**

**1. Data not normalized:**
```python
# MUST normalize XYZ coordinates
xyz_mean = xyz.mean(axis=0)
xyz_std = xyz.std(axis=0)
xyz_norm = (xyz - xyz_mean) / (xyz_std + 1e-8)
```

**2. Wrong class mapping:**
```python
# Check labels are 0-4, not 1-5 or other range
print(f"Label range: {labels.min()} - {labels.max()}")  # Should be 0-4
```

**3. Model too small:**
```python
# Ensure using PointNet++ (not SimplePointNet)
model = PointNet2(num_classes=5, num_features=7)  # Not SimplePointNet
```

---

### Getting Help

**Check documentation:**
- `YOUR_COMPLETE_PROJECT_GUIDE.md` - 80+ page comprehensive guide
- `FINAL_PROJECT_SUMMARY.md` - Executive summary
- `results/model_comparison.md` - Detailed model comparison

**Debug checklist:**
```
â–¡ GPU detected? (torch.cuda.is_available())
â–¡ Data normalized? (XYZ centered and scaled)
â–¡ Correct model? (PointNet++ for best results)
â–¡ Checkpoint exists? (checkpoints/pointnet2_best_model.pth)
â–¡ Labels in range 0-4? (print labels.min(), labels.max())
â–¡ Sufficient GPU memory? (6GB minimum)
```

---

## ğŸ“š References

### Papers

1. **PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space**
   - Qi, C. R., Yi, L., Su, H., & Guibas, L. J. (2017)
   - *NeurIPS 2017*
   - [arXiv:1706.02413](https://arxiv.org/abs/1706.02413)
   - **Our implementation**

2. **PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation**
   - Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017)
   - *CVPR 2017*
   - [arXiv:1612.00593](https://arxiv.org/abs/1612.00593)
   - **Baseline inspiration**

3. **RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds**
   - Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., ... & Markham, A. (2020)
   - *CVPR 2020*
   - [arXiv:1911.11236](https://arxiv.org/abs/1911.11236)
   - **Future work**

### Tools & Libraries

4. **PyTorch** - Deep learning framework
   - https://pytorch.org/
   - Version: 2.5.1+cu121

5. **CloudCompare** - Point cloud processing and labeling
   - https://www.cloudcompare.org/
   - Used for manual labeling

6. **laspy** - LAS file I/O
   - https://github.com/laspy/laspy
   - LAS format reading/writing

### Datasets

7. **SemanticKITTI** - Outdoor point cloud benchmark
   - http://semantic-kitti.org/
   - Comparison reference

8. **Toronto-3D** - Urban scene segmentation
   - https://github.com/WeikaiTan/Toronto-3D
   - Related work

### Guides & Tutorials

9. **YOUR_COMPLETE_PROJECT_GUIDE.md** (This Repository)
   - 80+ page comprehensive guide
   - Everything from basics to advanced concepts
   - **Read this for deep understanding**

10. **PointNet++ PyTorch Implementation**
    - https://github.com/yanx27/Pointnet_Pointnet2_pytorch
    - Reference implementation

---

## ğŸ“… Project Timeline

<div align="center">

**Total Duration: 8 Weeks (November 1 - December 25, 2025)**

</div>

### ğŸ“† Phase 1: Research & Planning
**November 1-15, 2025 (2 weeks)**

- âœ… Literature review of point cloud segmentation methods
- âœ… Evaluated PointNet, PointNet++, RandLA-Net architectures
- âœ… Defined project scope and target metrics (88-90% accuracy)
- âœ… Selected 5 semantic categories for MMS data
- âœ… Technology stack selection (PyTorch, CUDA)

**Deliverable:** Project proposal, architecture comparison

---

### ğŸ“† Phase 2: Data Collection & Labeling
**November 16-30, 2025 (2 weeks)**

- âœ… Acquired MMS point cloud data (LAS format)
- âœ… Manual labeling using CloudCompare
  - **Total labeled**: 1,461,189 points
  - **Classes**: Road, Snow, Vehicle, Vegetation, Others
- âœ… Quality control and validation
- âœ… Designed class mapping (LAS codes â†’ 5 target classes)

**Deliverable:** Labeled dataset (1.46M points)

---

### ğŸ“† Phase 3: Data Pipeline Development
**December 1-7, 2025 (1 week)**

- âœ… Implemented LAS file reading (laspy library)
- âœ… Created preprocessing pipeline
  - Normalization (XYZ centering and scaling)
  - Data augmentation (rotation, scaling)
- âœ… Developed train/val/test split (70/15/15)
- âœ… Built PyTorch DataLoader with batch processing

**Deliverable:** `prepare_training_data.py`, processed NPZ files

---

### ğŸ“† Phase 4: Baseline Implementation
**December 8-12, 2025 (5 days)**

- âœ… Implemented SimplePointNet architecture (192K parameters)
- âœ… Initial training experiments
- âœ… Hyperparameter tuning
- âœ… **Baseline result**: 86.01% accuracy
- âœ… Identified improvement areas (Vegetation: 61% IoU)

**Deliverable:** SimplePointNet model, 86% accuracy baseline

---

### ğŸ“† Phase 5: Advanced Model Development
**December 13-18, 2025 (6 days)**

- âœ… Implemented PointNet++ architecture (968K parameters)
- âœ… Debugged dimension mismatch issues
  - Fixed `in_channel` calculation
  - Added tensor permutations
- âœ… Integrated GPU acceleration
  - Installed CUDA PyTorch 2.5.1+cu121
  - Verified RTX 4050 utilization

**Deliverable:** Working PointNet++ implementation

---

### ğŸ“† Phase 6: Model Training & Optimization
**December 19-23, 2025 (5 days)**

- âœ… Trained PointNet++ for 30 epochs
  - GPU training: ~4 hours on RTX 4050
  - Implemented data augmentation
  - Learning rate scheduling (ReduceLROnPlateau)
- âœ… **Best validation IoU**: 94.05% (epoch 28)
- âœ… Saved best checkpoint

**Deliverable:** Trained PointNet++ model (94% val IoU)

---

### ğŸ“† Phase 7: Evaluation & Analysis
**December 24, 2025 (1 day)**

- âœ… Comprehensive test set evaluation
  - **Test accuracy**: 94.78%
  - **Mean IoU**: 87.51%
  - **Kappa**: 0.9187
- âœ… Generated confusion matrices
- âœ… Comparative analysis (SimplePointNet vs PointNet++)
- âœ… Created 6 comparison visualizations
- âœ… Per-class performance breakdown

**Deliverable:** Test results, evaluation metrics, visualizations

---

### ğŸ“† Phase 8: Documentation & Presentation
**December 25, 2025 (1 day)**

- âœ… Complete technical documentation
  - `YOUR_COMPLETE_PROJECT_GUIDE.md` (80+ pages)
  - `FINAL_PROJECT_SUMMARY.md`
  - Enhanced `README.md`
- âœ… Model comparison analysis
- âœ… Visualization generation (11 charts total)
- âœ… PowerPoint presentation (21 slides)
- âœ… GitHub repository finalization

**Deliverable:** Complete documentation, presentation materials

---

### ğŸ¯ Key Milestones

| Date | Milestone | Status |
|:----:|-----------|:------:|
| **Nov 15** | Project scope defined | âœ… |
| **Nov 30** | Data labeling complete (1.46M points) | âœ… |
| **Dec 7** | Data pipeline operational | âœ… |
| **Dec 12** | Baseline model (86% accuracy) | âœ… |
| **Dec 18** | PointNet++ implementation complete | âœ… |
| **Dec 23** | Final model training complete | âœ… |
| **Dec 24** | Test evaluation (94.78% accuracy) | âœ… |
| **Dec 25** | Project documentation complete | âœ… |

---

### ğŸ“Š Time Breakdown

```
Research & Planning:        â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (15%)  2 weeks
Data Collection:            â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (15%)  2 weeks
Data Pipeline:              â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (9%)   1 week
Baseline Implementation:    â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ (11%)  5 days
PointNet++ Development:     â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ (13%)  6 days
Training & Optimization:    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (14%)  5 days
Evaluation:                 â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (9%)   1 day
Documentation:              â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (14%)  1 day
                            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                      8 weeks (56 days)
```

**Effort Distribution:**
- Implementation: 38% (3 weeks)
- Data work: 30% (2.5 weeks)
- Research & planning: 15% (1.5 weeks)
- Evaluation & docs: 17% (1.5 weeks)

---

<div align="center">

## ğŸ“ Project Status

### âœ… **SUCCESSFULLY COMPLETED**

**Both SimplePointNet (86% accuracy) and PointNet++ (94.78% accuracy) are trained, evaluated, and ready for deployment.**

**All code, trained models, evaluation results, and comprehensive documentation are available in this repository.**

---

### ğŸ“¦ **Deliverables**

âœ… Working codebase (training + inference)
âœ… Trained models (SimplePointNet + PointNet++)
âœ… Labeled dataset (1.46M points)
âœ… Comprehensive evaluation (94.78% accuracy)
âœ… 11 visualizations and charts
âœ… 80+ page technical guide
âœ… Complete documentation

---

### ğŸš€ **Recommended Model**

**PointNet++** (`checkpoints/pointnet2_best_model.pth`)

- **Accuracy**: 94.78%
- **Mean IoU**: 87.51%
- **Training**: Epoch 28
- **Status**: Production-ready

---

### ğŸ† **Achievement Unlocked**

**Exceeded target accuracy by 4.78% to 6.78%**

Target: 88-90% | **Achieved: 94.78%** âœ¨

---

### ğŸ“§ Contact & Support

**For questions, issues, or collaboration:**

Create an issue in this repository

---

### â­ If you found this project helpful, please star the repository!

[![GitHub stars](https://img.shields.io/github/stars/nikengoswami/MMS-Point-Cloud-Classification?style=social)]()

---

**Built with â¤ï¸ using PyTorch, PointNet++, and determination**

**December 2025**

</div>
